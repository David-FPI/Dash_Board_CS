import streamlit as st
import pandas as pd
import re
from unidecode import unidecode

st.set_page_config(page_title="üìä ƒê·ªçc t√™n nh√¢n vi√™n & T√≠nh KPI", page_icon="üë©‚Äçüíº")

# =====================
# üîß C√°c keyword linh ho·∫°t ƒë·ªÉ match c√°c c·ªôt
COLUMN_KEYWORDS = {
    "T∆∞∆°ng t√°c ‚â•10 c√¢u": [">=10", "‚â•10", "tuong tac", "so tuong tac", "t∆∞∆°ng t√°c"],
    "L∆∞·ª£ng tham gia group Zalo": ["group zalo", "tham gia group", "luong tham gia", "zalo nhom", "zalo group", "nhom zalo", "zalo", "tham gia zalo", "zalo tham gia", "zalo group join", "nh·∫≠u zalo", "Âä†ÂÖ•zaloÁæ§Êï∞Èáè"],
    "T·ªïng s·ªë k·∫øt b·∫°n trong ng√†y": ["ket ban", "so ket ban", "tong ket ban", "tong so ket ban", "ket ban trong ngay", "zalo", "ket ban zalo", "ngay", "ketban", "ÂΩìÂ§©Âä†zalo"]
}

# =====================
# üîß Chu·∫©n h√≥a text ƒë·ªÉ match header

def normalize_text(text):
    text = str(text).replace("\n", " ").replace("\r", " ")
    text = unidecode(text)
    text = re.sub(r"\s+", " ", text)
    return text.strip().lower()

# =====================
# üîç T√¨m v·ªã tr√≠ c·ªôt theo keyword

def match_column_indices(header_row):
    mapping = {}
    for idx, col in enumerate(header_row):
        col_clean = normalize_text(col)
        for target_name, keyword_list in COLUMN_KEYWORDS.items():
            if any(kw in col_clean for kw in keyword_list):
                mapping[target_name] = idx
    return mapping

# =====================
# üìÖ ƒê·ªçc d·ªØ li·ªáu t·ª´ sheet

def extract_data_from_sheet(df, sheet_name):
    data = []
    if df.shape[0] < 5:
        return data, []

    df.columns = range(df.shape[1])  # reset column index
    df = df.drop([0, 1])  # B·ªè 2 d√≤ng ƒë·∫ßu
    header = df.iloc[0]
    df = df[1:].reset_index(drop=True)
    col_map = match_column_indices(header)

    if len(col_map) < 2:
        return [], list(col_map.keys())

    df[1] = df[1].fillna(method="ffill")
    current_nv = None
    empty_count = 0

    for i in range(df.shape[0]):
        row = df.iloc[i]

        if pd.notna(row[1]):
            name_cell = str(row[1]).strip()
            if name_cell.lower() in ["nhan vien", "tong", "stat"]:
                continue
            current_nv = re.sub(r"\(.*?\)", "", name_cell).strip()

        if not current_nv:
            continue

        nguon = str(row[2]).strip() if pd.notna(row[2]) else ""
        if nguon == "" or nguon.lower() == "nan":
            empty_count += 1
            if empty_count >= 2:
                break
            continue
        else:
            empty_count = 0

        data.append({
            "Nh√¢n vi√™n": current_nv,
            "Ngu·ªìn": nguon,
            "T∆∞∆°ng t√°c ‚â•10 c√¢u": row.get(col_map.get("T∆∞∆°ng t√°c ‚â•10 c√¢u")),
            "L∆∞·ª£ng tham gia group Zalo": row.get(col_map.get("L∆∞·ª£ng tham gia group Zalo")),
            "T·ªïng s·ªë k·∫øt b·∫°n trong ng√†y": row.get(col_map.get("T·ªïng s·ªë k·∫øt b·∫°n trong ng√†y")),
            "Sheet": sheet_name
        })
    return data, list(col_map.keys())

# =====================
# üìÇ ƒê·ªçc to√†n b·ªô file Excel

def extract_all_data(file):
    xls = pd.ExcelFile(file)
    all_rows = []
    warnings = []
    for sheet in xls.sheet_names:
        df = pd.read_excel(xls, sheet_name=sheet, header=None)
        records, found_cols = extract_data_from_sheet(df, sheet)
        all_rows.extend(records)
        if len(found_cols) < 2:
            warnings.append((sheet, found_cols))
    return pd.DataFrame(all_rows), warnings

# =====================
# üîç App
st.title("üìÖ ƒê·ªçc T√™n Nh√¢n Vi√™n & T√≠nh KPI T·ª´ File Excel B√°o C√°o")
uploaded_files = st.file_uploader("K√©o & th·∫£ nhi·ªÅu file Excel v√†o ƒë√¢y", type=["xlsx"], accept_multiple_files=True)

if uploaded_files:
    all_data = []
    all_warnings = []
    for file in uploaded_files:
        st.write(f"üìÇ ƒêang x·ª≠ l√Ω: `{file.name}`")
        df, warns = extract_all_data(file)
        all_data.append(df)
        all_warnings.extend(warns)

    df_all = pd.concat(all_data, ignore_index=True)

    if not df_all.empty:
        df_all["Nh√¢n vi√™n chu·∫©n"] = df_all["Nh√¢n vi√™n"].apply(lambda x: str(x).strip().title())

        st.subheader("‚úÖ Danh s√°ch Nh√¢n vi√™n ƒë√£ chu·∫©n h√≥a")
        st.dataframe(df_all[["Nh√¢n vi√™n", "Nh√¢n vi√™n chu·∫©n", "Sheet"]].drop_duplicates(), use_container_width=True)

        st.success(f"‚úÖ T·ªïng s·ªë d√≤ng d·ªØ li·ªáu: {len(df_all)} ‚Äî üë©‚Äçüíº Nh√¢n vi√™n duy nh·∫•t: {df_all['Nh√¢n vi√™n chu·∫©n'].nunique()}")

        # C·∫£nh b√°o sheet b·ªã thi·∫øu KPI
        for sheet, found in all_warnings:
            st.warning(f"‚ö†Ô∏è Sheet {sheet} kh√¥ng ƒë·ªß c·ªôt KPI ‚Äî d√≤ ƒë∆∞·ª£c {found}")
    else:
        st.error("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu h·ª£p l·ªá. Ki·ªÉm tra file.")

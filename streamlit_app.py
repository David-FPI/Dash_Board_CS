import streamlit as st
import pandas as pd
import re

st.set_page_config(page_title="Tá»•ng há»£p bÃ¡o cÃ¡o nhÃ¢n viÃªn", layout="wide")
st.title("ğŸ“Š Tá»•ng há»£p bÃ¡o cÃ¡o nhÃ¢n viÃªn tá»« nhiá»u file Excel")

# ======================== HÃ m chuáº©n hÃ³a & xá»­ lÃ½ ========================
def normalize_staff_name(name):
    if not isinstance(name, str):
        return ""
    name = re.sub(r"\(.*?\)", "", name)
    name = re.sub(r"\s+", " ", name).strip()
    return name

def extract_header(df):
    if len(df) < 3:
        raise ValueError("File thiáº¿u dÃ²ng tiÃªu Ä‘á» (Ã­t hÆ¡n 3 dÃ²ng).")
    row1 = df.iloc[1].fillna("")
    row2 = df.iloc[2].fillna("")
    header = row1.astype(str) + " " + row2.astype(str)
    header = header.str.replace(r"\s+", " ", regex=True).str.strip()
    return header

def extract_data_block(df_raw):
    header = extract_header(df_raw)
    header = pd.Series(header)
    header = header.where(~header.duplicated(), header + "_" + header.groupby(header).cumcount().astype(str))
    df_data = df_raw.iloc[3:].copy()

    cutoff_idx = df_data[df_data.iloc[:, 0].astype(str).str.contains("ç»Ÿè®¡|Tá»•ng", case=False, na=False)].index
    if not cutoff_idx.empty:
        df_data = df_data.loc[:cutoff_idx[0] - 1]

    df_data.columns = header
    df_data.reset_index(drop=True, inplace=True)

    staff_col = next((c for c in df_data.columns if "nhÃ¢n viÃªn" in c.lower()), None)
    if not staff_col:
        raise ValueError("KhÃ´ng tÃ¬m tháº¥y cá»™t NhÃ¢n viÃªn.")

    last_name = None
    for i in range(len(df_data)):
        name = df_data.at[i, staff_col]
        if pd.notna(name) and str(name).strip() != "":
            last_name = normalize_staff_name(name)
        elif last_name:
            df_data.at[i, staff_col] = last_name

    return df_data

# ======================== Xá»­ lÃ½ nhiá»u sheet ========================
def process_all_sheets(file):
    xls = pd.ExcelFile(file)
    all_data = []
    log = []
    for sheet in xls.sheet_names:
        try:
            df_raw = xls.parse(sheet, header=None)
            cleaned = extract_data_block(df_raw)
            cleaned["__Sheet__"] = sheet
            all_data.append(cleaned)
            log.append({"Sheet": sheet, "Status": "âœ… ÄÃ£ xá»­ lÃ½", "Rows": len(cleaned)})
        except Exception as e:
            log.append({"Sheet": sheet, "Status": f"âŒ Bá» qua - {str(e)}", "Rows": 0})
    log_df = pd.DataFrame(log)
    return pd.concat(all_data, ignore_index=True) if all_data else pd.DataFrame(), log_df

# ======================== Chuáº©n hÃ³a tÃªn cá»™t ========================
def normalize_column_name(col):
    col = str(col)
    col = re.sub(r"\s+", " ", col)
    col = col.strip().lower()
    return col

# ======================== Giao diá»‡n ========================
uploaded_files = st.file_uploader("ğŸ“ Táº£i lÃªn 1 hoáº·c nhiá»u file bÃ¡o cÃ¡o Excel", type=["xlsx"], accept_multiple_files=True)

if uploaded_files:
    full_data = []
    for file in uploaded_files:
        try:
            st.success(f"âœ”ï¸ Äang xá»­ lÃ½: {file.name}")
            df_all, sheet_log = process_all_sheets(file)
            df_all["__File__"] = file.name
            full_data.append(df_all)
        except Exception as e:
            st.error(f"âŒ Lá»—i khi xá»­ lÃ½ file {file.name}: {e}")

    if full_data:
        df_final = pd.concat(full_data, ignore_index=True)
        # ğŸ” In thá»­ cÃ¡c sheet vÃ  sá»‘ cá»™t nháº­n Ä‘Æ°á»£c tá»« má»—i sheet
        st.markdown("### ğŸ“Œ Check: Cá»™t nháº­n Ä‘Æ°á»£c tá»« má»—i sheet")

        if "__Sheet__" in df_final.columns:
            sheet_col_map = df_final.groupby("__Sheet__").agg(lambda x: list(x.index)).reset_index()
            sheet_col_map["Sá»‘ dÃ²ng"] = sheet_col_map["__Sheet__"].apply(lambda sheet: len(df_final[df_final["__Sheet__"] == sheet]))
            sheet_col_map["Sá»‘ cá»™t"] = sheet_col_map["__Sheet__"].apply(lambda sheet: df_final[df_final["__Sheet__"] == sheet].shape[1])
            st.dataframe(sheet_col_map[["__Sheet__", "Sá»‘ dÃ²ng", "Sá»‘ cá»™t"]], use_container_width=True)

            # Optional: hiá»ƒn thá»‹ 3 dÃ²ng Ä‘áº§u cá»§a má»—i sheet
            for sheet in df_final["__Sheet__"].unique():
                st.markdown(f"#### ğŸ§¾ Sheet: `{sheet}` - 3 dÃ²ng Ä‘áº§u")
                st.dataframe(df_final[df_final["__Sheet__"] == sheet].head(3), use_container_width=True)
        else:
            st.warning("âš ï¸ KhÃ´ng tÃ¬m tháº¥y cá»™t '__Sheet__'. CÃ³ thá»ƒ hÃ m process_all_sheets() Ä‘ang bá»‹ lá»—i.")




        st.subheader("âœ… Dá»¯ liá»‡u Ä‘Ã£ tá»•ng há»£p")
        st.dataframe(df_final.head(50), use_container_width=True)

        # â€”â€”â€”â€”â€”â€” START: TÃNH KPI â€”â€”â€”â€”â€”â€”
        normalized_cols = {c: normalize_column_name(c) for c in df_final.columns}

        kpi_ketban_keywords = ["tá»•ng sá»‘ káº¿t báº¡n trong ngÃ y", "å½“å¤©åŠ zaloæ€»æ•°"]
        kpi_tuongtac_keywords = ["tÆ°Æ¡ng tÃ¡c â‰¥10 cÃ¢u", "â‰¥10"]
        kpi_groupzalo_keywords = ["tham gia group zalo", "lÆ°á»£ng tham gia group zalo"]

        def find_cols_by_keywords(keywords):
            return [orig for orig, norm in normalized_cols.items() if any(kw in norm for kw in keywords)]

        def find_col_by_keywords(keywords):
            return next((orig for orig, norm in normalized_cols.items()
                        if any(kw in norm for kw in keywords)), None)

        cols_ketban = find_cols_by_keywords(kpi_ketban_keywords)
        cols_tuongtac = find_cols_by_keywords(kpi_tuongtac_keywords)
        cols_groupzalo = find_cols_by_keywords(kpi_groupzalo_keywords)

        if not (cols_ketban and cols_tuongtac and cols_groupzalo):
            st.warning("âš ï¸ KhÃ´ng tÃ¬m Ä‘á»§ 3 cá»™t KPI (káº¿t báº¡n, tÆ°Æ¡ng tÃ¡c, group Zalo). Vui lÃ²ng kiá»ƒm tra láº¡i tÃªn cá»™t.")
        else:
            df_final["kpi_ketban"] = df_final[cols_ketban].apply(pd.to_numeric, errors="coerce").fillna(0).sum(axis=1)
            df_final["kpi_tuongtac"] = df_final[cols_tuongtac].apply(pd.to_numeric, errors="coerce").fillna(0).sum(axis=1)
            df_final["kpi_groupzalo"] = df_final[cols_groupzalo].apply(pd.to_numeric, errors="coerce").fillna(0).sum(axis=1)

            # ğŸ¯ NÃ¢ng cáº¥p tÃ¬m cá»™t NhÃ¢n viÃªn vÃ  Nguá»“n
            staff_keywords = ["nhÃ¢n viÃªn", "äººå‘˜", "æˆå‘˜"]
            source_keywords = ["nguá»“n", "æ¸ é“"]

            staff_col = find_col_by_keywords(staff_keywords)
            source_col = find_col_by_keywords(source_keywords)

            kpi_cols = ["kpi_ketban", "kpi_tuongtac", "kpi_groupzalo"]

            df_kpi = df_final.groupby([staff_col, source_col], as_index=False)[kpi_cols].sum()
            st.subheader("ğŸ“ˆ KPI theo nhÃ¢n viÃªn vÃ  nguá»“n")
            st.dataframe(df_kpi, use_container_width=True)

            df_kpi_total = df_kpi.groupby(staff_col, as_index=False)[kpi_cols].sum()
            df_kpi_total["Hiá»‡u suáº¥t (%)"] = df_kpi_total.apply(
                lambda row: (row["kpi_groupzalo"] / row["kpi_ketban"] * 100) if row["kpi_ketban"] != 0 else None,
                axis=1
            )
            df_kpi_total["Hiá»‡u suáº¥t (%)"] = pd.to_numeric(df_kpi_total["Hiá»‡u suáº¥t (%)"], errors="coerce").round(2)


            st.subheader("ğŸ“Š KPI tá»•ng há»£p theo nhÃ¢n viÃªn")
            st.dataframe(df_kpi_total, use_container_width=True)

        # â€”â€”â€”â€”â€”â€” END: TÃNH KPI â€”â€”â€”â€”â€”â€”

        csv = df_final.to_csv(index=False).encode('utf-8-sig')
        st.download_button("ğŸ“¥ Táº£i dá»¯ liá»‡u tá»•ng há»£p CSV", csv, "tong_hop_bao_cao.csv", "text/csv")
